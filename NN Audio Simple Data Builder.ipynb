{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a long file with clip FT magnitudes transposed and concatenated into a long series of samples\n",
    "\n",
    "It's designed for a process that just grabs a window instead of using a prepared frame (since that creates massive data duplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import cmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shared constants and functions\n",
    "%run \"NN Audio Core.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of all zero samples between clips\n",
    "# TODO - not using pads at all, on assumption that the samples have a bit of buffer at start and end\n",
    "#PAD_SIZE = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1025, 239)\n"
     ]
    }
   ],
   "source": [
    "# some test data to hack around with\n",
    "test_file = \"Assets\\\\DataShareArchive\\\\Test\\\\Clean\\\\p232_010.wav\"\n",
    "wav, rate = librosa.core.load(test_file)\n",
    "fft = get_ft(wav)\n",
    "print(fft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data from clip wave files for adding to long data arrays\n",
    "\n",
    "# Sample output is (samples, bins) all converted to magnitude\n",
    "def get_samples(file):\n",
    "    wav, rate = librosa.core.load(file)\n",
    "    samples = abs(get_ft(wav).T) # organized as bins, frames so we need to transpose them to frames, bins\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_file(data_path, max_samples):\n",
    "    frames_file = data_path + \"\\\\fsamples-\" + str(max_samples)\n",
    "    filename = os.fsdecode(frames_file)\n",
    "    return filename\n",
    "\n",
    "def targets_file(data_path, max_samples):\n",
    "    targets_file = data_path + \"\\\\ftargets-\" + str(max_samples)\n",
    "    filename = os.fsdecode(targets_file)\n",
    "    return filename\n",
    "\n",
    "# Iterate over clean & noisy folders to create frames and targets\n",
    "# Updated to select clips based on difference between clean & noisy versions\n",
    "def create_data(wav_root, data_path, max_samples = 10000, min_diff = 70, max_diff=300):\n",
    "    clean_dir = wav_root + \"\\\\Clean\\\\\"\n",
    "    noisy_dir = wav_root + \"\\\\Noisy\\\\\"\n",
    "    sample_index = 0\n",
    "    frames = np.memmap(frames_file(data_path, max_samples), mode='w+', dtype=DTYPE, shape=(max_samples,FFT_BINS,1))\n",
    "    targets = np.memmap(targets_file(data_path, max_samples), mode='w+', dtype=DTYPE, shape=(max_samples,FFT_BINS))\n",
    "#    frames = np.empty(dtype=DTYPE, shape=(max_samples,FFT_BINS,1))\n",
    "#    targets = np.empty(dtype=DTYPE, shape=(max_samples,FFT_BINS))\n",
    "    file_list = os.listdir(clean_dir)\n",
    "    file_index = 0\n",
    "    while (sample_index < max_samples) and (file_index < len(file_list)) :\n",
    "        file = file_list[file_index]\n",
    "        filename = os.fsdecode(file)\n",
    "        noisy_ft = get_ft_from_file(noisy_dir + file)\n",
    "        clean_ft = get_ft_from_file(clean_dir + file)\n",
    "        diff = diff_ft(noisy_ft, clean_ft)\n",
    "        if (diff > min_diff) and (diff < max_diff):\n",
    "            new_frames = get_samples(noisy_dir + file)\n",
    "            max_step = min(new_frames.shape[0], max_samples-sample_index)\n",
    "            frames[sample_index:sample_index+max_step,:,0] = new_frames[:max_step,:]\n",
    "            new_targets = get_samples(clean_dir + file)\n",
    "            targets[sample_index:sample_index+max_step,:] = new_targets[:max_step,:]\n",
    "            sample_index += new_targets.shape[0]\n",
    "        file_index += 1\n",
    "    print(\"Reached sample # \" + str(min(sample_index, max_samples)))\n",
    "    return frames, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached sample # 500000\n"
     ]
    }
   ],
   "source": [
    "# small test data \"Assets\\\\DataShareArchive\\\\Test\"\n",
    "# 28K \"F:\\\\Audiodata\\\\Train28Spk\"\n",
    "f, t = create_data(\"F:\\\\Audiodata\\\\Train28Spk\", \"f:\\\\Audiodata\", max_samples=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

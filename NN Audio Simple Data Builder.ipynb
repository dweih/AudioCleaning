{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a long file with clip FT magnitudes transposed and concatenated into a long series of samples\n",
    "\n",
    "It's designed for a process that just grabs a window instead of using a prepared frame (since that creates massive data duplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import cmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and settings\n",
    "DTYPE = 'float32'\n",
    "\n",
    "# Number of all zero samples between clips\n",
    "# TODO - not using pads at all, on assumption that the samples have a bit of buffer at start and end\n",
    "# PAD_SIZE = 1 \n",
    "\n",
    "# cqt related\n",
    "FFT_BINS = 768 # function of items below\n",
    "HOP_LENGTH = 256 # Required for good cqt results\n",
    "\n",
    "# stft values\n",
    "N_FFT = 1024 # 512 recommended for speech, music typically 2048\n",
    "\n",
    "# cqt values\n",
    "BINS_PER_OCTAVE = 12 * 8\n",
    "FMIN = librosa.note_to_hz('C1')\n",
    "OCTAVES = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea here is that we operate on magnitude, and will just use phase from the original noisy sample\n",
    "\n",
    "def rebuild_fft(output, original_fft):\n",
    "    vphase = np.vectorize(cmath.phase)\n",
    "    o_phase = vphase(original_fft)\n",
    "    mag = output.T\n",
    "    vrect = np.vectorize(cmath.rect)\n",
    "    return vrect(mag, o_phase)\n",
    "    \n",
    "# May not actually use this - may want to just pass a reduced view and then add this back to get right shape\n",
    "def filter(cqt):\n",
    "    cqt[0:BINS_PER_OCTAVE,:] = 0\n",
    "    return cqt\n",
    "\n",
    "def get_ft(wav):\n",
    "    c = librosa.cqt(wav, hop_length=HOP_LENGTH, fmin=FMIN, n_bins=OCTAVES*BINS_PER_OCTAVE, bins_per_octave=BINS_PER_OCTAVE)\n",
    "    #c = librosa.stft(wav, hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "    return c\n",
    "\n",
    "def inv_ft(ft):\n",
    "    return librosa.icqt(ft, hop_length=HOP_LENGTH, fmin=FMIN, bins_per_octave=BINS_PER_OCTAVE)\n",
    "    #return librosa.istft(ft, hop_length=HOP_LENGTH)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 239)\n"
     ]
    }
   ],
   "source": [
    "# some test data to hack around with\n",
    "test_file = \"Assets\\\\DataShareArchive\\\\Test\\\\Clean\\\\p232_010.wav\"\n",
    "wav, rate = librosa.core.load(test_file)\n",
    "fft = get_ft(wav)\n",
    "print(fft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data from clip wave files for adding to long data arrays\n",
    "\n",
    "# Sample output is (samples, bins) all converted to magnitude\n",
    "def get_samples(file):\n",
    "    wav, rate = librosa.core.load(file)\n",
    "    samples = abs(get_ft(wav).T) # organized as bins, frames so we need to transpose them to frames, bins\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_file(data_path, max_samples):\n",
    "    frames_file = data_path + \"\\\\fsamples-\" + str(max_samples)\n",
    "    filename = os.fsdecode(frames_file)\n",
    "    return filename\n",
    "\n",
    "def targets_file(data_path, max_samples):\n",
    "    targets_file = data_path + \"\\\\ftargets-\" + str(max_samples)\n",
    "    filename = os.fsdecode(targets_file)\n",
    "    return filename\n",
    "\n",
    "# Iterate over clean & noisy folders to create frames and targets\n",
    "def create_data(wav_root, data_path, max_samples = 10000):\n",
    "    clean_dir = wav_root + \"\\\\Clean\\\\\"\n",
    "    noisy_dir = wav_root + \"\\\\Noisy\\\\\"\n",
    "    sample_index = 0\n",
    "    frames = np.memmap(frames_file(data_path, max_samples), mode='w+', dtype=DTYPE, shape=(max_samples,FFT_BINS,1))\n",
    "    targets = np.memmap(targets_file(data_path, max_samples), mode='w+', dtype=DTYPE, shape=(max_samples,FFT_BINS))\n",
    "#    frames = np.empty(dtype=DTYPE, shape=(max_samples,FFT_BINS,1))\n",
    "#    targets = np.empty(dtype=DTYPE, shape=(max_samples,FFT_BINS))\n",
    "    file_list = os.listdir(clean_dir)\n",
    "    file_index = 0\n",
    "    while (sample_index < max_samples) and (file_index < len(file_list)) :\n",
    "        file = file_list[file_index]\n",
    "        filename = os.fsdecode(file)\n",
    "        new_frames = get_samples(noisy_dir + file)\n",
    "        max_step = min(new_frames.shape[0], max_samples-sample_index)\n",
    "        frames[sample_index:sample_index+max_step,:,0] = new_frames[:max_step,:]\n",
    "        new_targets = get_samples(clean_dir + file)\n",
    "        targets[sample_index:sample_index+max_step,:] = new_targets[:max_step,:]\n",
    "        sample_index += new_targets.shape[0]\n",
    "        file_index += 1\n",
    "    print(\"Reached sample # \" + str(min(sample_index, max_samples)))\n",
    "    return frames, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached sample # 300000\n"
     ]
    }
   ],
   "source": [
    "# Training data comes in at 357K samples total (at hop length 128)\n",
    "# small test data \"Assets\\\\DataShareArchive\\\\Test\"\n",
    "# 28K \"F:\\\\Audiodata\\\\Train28Spk\"\n",
    "f, t = create_data(\"F:\\\\Audiodata\\\\Train28Spk\", \"f:\\\\Audiodata\", max_samples=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

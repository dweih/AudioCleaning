{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a long file with clips glued together and 25 sample all zero buffers between them\n",
    "\n",
    "It's designed for a process that just grabs a window instead of using a prepared frame (since that creates massive data duplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio, display\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import cmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and settings\n",
    "DTYPE = 'float32'\n",
    "\n",
    "# Number of all zero samples between clips\n",
    "# TODO - not using pads at all, on assumption that the samples have a bit of buffer at start and end\n",
    "# PAD_SIZE = 1 \n",
    "\n",
    "# cqt related\n",
    "FFT_BINS = 768 # function of items below\n",
    "HOP_LENGTH = 128 # Required for good cqt results\n",
    "\n",
    "# stft values\n",
    "N_FFT = 1024 # 512 recommended for speech, music typically 2048\n",
    "\n",
    "# cqt values\n",
    "BINS_PER_OCTAVE = 12 * 8\n",
    "FMIN = librosa.note_to_hz('C1')\n",
    "OCTAVES = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea here is that we operate on magnitude, and will just use phase from the original noisy sample\n",
    "\n",
    "def rebuild_fft(output, original_fft):\n",
    "    vphase = np.vectorize(cmath.phase)\n",
    "    o_phase = vphase(original_fft)\n",
    "    mag = output.T\n",
    "    vrect = np.vectorize(cmath.rect)\n",
    "    return vrect(mag, o_phase)\n",
    "    \n",
    "# May not actually use this - may want to just pass a reduced view and then add this back to get right shape\n",
    "def filter(cqt):\n",
    "    cqt[0:BINS_PER_OCTAVE,:] = 0\n",
    "    return cqt\n",
    "\n",
    "def get_ft(wav):\n",
    "    c = librosa.cqt(wav, hop_length=HOP_LENGTH, fmin=FMIN, n_bins=OCTAVES*BINS_PER_OCTAVE, bins_per_octave=BINS_PER_OCTAVE)\n",
    "    #c = librosa.stft(wav, hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "    return c\n",
    "\n",
    "def inv_ft(ft):\n",
    "    return librosa.icqt(ft, hop_length=HOP_LENGTH, fmin=FMIN, bins_per_octave=BINS_PER_OCTAVE)\n",
    "    #return librosa.istft(ft, hop_length=HOP_LENGTH)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 477)\n"
     ]
    }
   ],
   "source": [
    "# some test data to hack around with\n",
    "test_file = \"Assets\\\\DataShareArchive\\\\Test\\\\Clean\\\\p232_010.wav\"\n",
    "wav, rate = librosa.core.load(test_file)\n",
    "fft = get_ft(wav)\n",
    "print(fft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data from clip wave files for adding to long data arrays\n",
    "\n",
    "# Sample output is (samples, bins) all converted to magnitude\n",
    "def get_samples(file):\n",
    "    wav, rate = librosa.core.load(file)\n",
    "    samples = abs(get_ft(wav).T) # organized as bins, frames so we need to transpose them to frames, bins\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_file(data_path, max_samples):\n",
    "    frames_file = data_path + \"\\\\fsamples-\" + str(max_samples)\n",
    "    filename = os.fsdecode(frames_file)\n",
    "    return filename\n",
    "\n",
    "def targets_file(data_path, max_samples):\n",
    "    targets_file = data_path + \"\\\\ftargets-\" + str(max_samples)\n",
    "    filename = os.fsdecode(targets_file)\n",
    "    return filename\n",
    "\n",
    "# Iterate over clean & noisy folders to create frames and targets\n",
    "def create_data(wav_root, data_path, max_samples = 10000):\n",
    "    clean_dir = wav_root + \"\\\\Clean\\\\\"\n",
    "    noisy_dir = wav_root + \"\\\\Noisy\\\\\"\n",
    "    sample_index = 0\n",
    "#    frames = np.memmap(frames_file(data_path, max_samples), mode='w+', dtype=DTYPE, shape=(max_samples,FFT_BINS,1))\n",
    "#    targets = np.memmap(targets_file(data_path, max_samples), mode='w+', dtype=DTYPE, shape=(max_samples,FFT_BINS))\n",
    "    frames = np.empty(dtype=DTYPE, shape=(max_samples,FFT_BINS,1))\n",
    "    targets = np.empty(dtype=DTYPE, shape=(max_samples,FFT_BINS))\n",
    "    file_list = os.listdir(clean_dir)\n",
    "    file_index = 0\n",
    "    while (sample_index < max_samples) and (file_index < len(file_list)) :\n",
    "        file = file_list[file_index]\n",
    "        filename = os.fsdecode(file)\n",
    "        new_frames = get_samples(noisy_dir + file)\n",
    "        max_step = min(new_frames.shape[0], max_samples-sample_index)\n",
    "        frames[sample_index:sample_index+max_step,:,0] = new_frames[:max_step,:]\n",
    "        new_targets = get_samples(clean_dir + file)\n",
    "        targets[sample_index:sample_index+max_step,:] = new_targets[:max_step,:]\n",
    "        sample_index += new_targets.shape[0]\n",
    "        file_index += 1\n",
    "    print(\"Reached sample # \" + str(min(sample_index, max_samples)))\n",
    "    return frames, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last frame  [[0.00290483 0.08943819 0.0277468  0.03653067 0.00570014 0.04190635\n",
      "  0.05481525 0.00064881 0.01351169 0.02498227 0.0180965  0.0258149\n",
      "  0.08558448 0.00326359 0.03970534 0.01028745 0.0360076  0.03015906\n",
      "  0.00171996 0.05052185]]\n",
      "last frame  [[0.05407887 0.04926184 0.0225051  0.04978212 0.00277668 0.02292588\n",
      "  0.02362183 0.00177662 0.05377227 0.02528083 0.06121838 0.04865208\n",
      "  0.01945572 0.02569535 0.0156109  0.02349244 0.01097936 0.00673081\n",
      "  0.01102477 0.00298993]]\n",
      "last frame  [[0.02338694 0.02601526 0.00943256 0.00870871 0.03182755 0.03828942\n",
      "  0.00936001 0.00791292 0.00025157 0.02614782 0.01238661 0.00750359\n",
      "  0.0012963  0.0124521  0.00298445 0.00780138 0.00216881 0.01003475\n",
      "  0.00933276 0.00115248]]\n",
      "Reached sample # 2000\n"
     ]
    }
   ],
   "source": [
    "# Training data comes in at ~ 4 * 85K samples total (changed hop size so this will have changed)\n",
    "f, t = create_data(\"Assets\\\\DataShareArchive\\\\Test\", \"f:\\\\Audiodata\", max_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print (f[20,200:210,0])\n",
    "print (t[20,0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((3,4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.ones((3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,:,0] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

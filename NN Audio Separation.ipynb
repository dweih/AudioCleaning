{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NN Audio Test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ugJNGnNlkTwI",
        "outputId": "7d3654f4-c212-4732-ce90-51c181daed3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio, display\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import time\n",
        "import cmath\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "from keras import regularizers\n",
        "from keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cPWfsAnkkgxD",
        "outputId": "3808ecac-a09f-4e04-ee66-677e8868b761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# If on google colab, run this\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('drive/My Drive/Projects/Audio Separation/Data')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X4ZOOJsSkTwM"
      },
      "source": [
        "Rewriten to use a single stream of data and on the fly creation of batch data\n",
        "\n",
        "TODO\n",
        "* Add saving best models\n",
        "\n",
        "* Change name from frames to samples everywhere\n",
        "\n",
        "* Go back to cqt??\n",
        "\n",
        "* Add drop out\n",
        "\n",
        "* Use callbacks for LR reduction and to save best models with error info\n",
        "\n",
        "* Automatic logging of graphs, errors, models etc. for comparison\n",
        "* Add graph of model\n",
        "    from keras.utils import plot_model\n",
        "    plot_model(model, to_file='model.png')\n",
        "* Create log output function, then capture to text file (also export graphs?)\n",
        "\n",
        "* Look for sources of noise that are close to what I want?  Or generate some clips?\n",
        "\n",
        "* Debugging tools for comparing, listening to, and viewing clips\n",
        "* Add audio quality comparisons between clean clips and cleaned clips for evaluation\n",
        "\n",
        "* Add history error plotting to compare different learning models, topologies, etc.\n",
        "\n",
        "* Look into streaming frames to & from file and having way more samples\n",
        "\n",
        "* Why does https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785 only use real component?\n",
        "* Evaluate different representations of complex numbers in terms of learnability\n",
        "\n",
        "* Consider generating custom data for goal - male speakers, low voice, specific sounds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a0NWhq04kTwN",
        "colab": {}
      },
      "source": [
        "# Constants and settings\n",
        "WINDOW_SIZE = 55  # Has to be odd\n",
        "TARGET_COL = WINDOW_SIZE//2\n",
        "\n",
        "DTYPE = 'float32'\n",
        "\n",
        "# stft values\n",
        "N_FFT = 1024 # 512 recommended for speech, music typically 2048\n",
        "FFT_BINS = 513\n",
        "\n",
        "# cqt values\n",
        "#FFT_BINS = 768 # function of items below\n",
        "HOP_LENGTH = 256\n",
        "\n",
        "BINS_PER_OCTAVE = 12 * 8\n",
        "FMIN = librosa.note_to_hz('C1')\n",
        "OCTAVES = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MUhAB8HrkTwP",
        "colab": {}
      },
      "source": [
        "# Idea here is that we operate on magnitude, and will just use phase from the original noisy sample\n",
        "\n",
        "def combine_target(t):\n",
        "    return (t[0:t.shape[0]//2] + 1j * t[t.shape[0]//2:]).reshape(1,(t.shape[0]//2))\n",
        "\n",
        "def rebuild_fft(output, original_fft):\n",
        "    vphase = np.vectorize(cmath.phase)\n",
        "    o_phase = vphase(original_fft)\n",
        "    mag = output.T\n",
        "    vrect = np.vectorize(cmath.rect)\n",
        "    return vrect(mag, o_phase)\n",
        "    \n",
        "# build up as (bins, samples) then transpose to model view of (samples, bins)\n",
        "def targets_to_fft(targets):\n",
        "    fft = np.empty((targets.shape[0],targets.shape[1]//2), dtype='complex64')\n",
        "    for i in range(0, targets.shape[0]):\n",
        "        fft[i] = combine_target(targets[i])\n",
        "    return fft.T   # transpose\n",
        "\n",
        "def get_ft(wav):\n",
        "    c = librosa.cqt(wav, hop_length=HOP_LENGTH, fmin=FMIN, n_bins=OCTAVES*BINS_PER_OCTAVE, bins_per_octave=BINS_PER_OCTAVE)\n",
        "    #c = librosa.stft(wav, hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
        "    return c\n",
        "\n",
        "def inv_ft(ft):\n",
        "    return librosa.icqt(ft, hop_length=HOP_LENGTH, bins_per_octave=BINS_PER_OCTAVE)\n",
        "    #return librosa.istft(ft, hop_length=HOP_LENGTH)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0mgM7ZVjlStg",
        "colab": {}
      },
      "source": [
        "# For local data\n",
        "samples=100000\n",
        "frames = np.memmap(\"f://audiodata//fsamples-\"+str(samples), mode='r', dtype=DTYPE, shape=(samples, FFT_BINS, 1))\n",
        "targets = np.memmap(\"f://audiodata//ftargets-\"+str(samples), mode='r', dtype=DTYPE, shape=(samples, FFT_BINS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xFRoJVK1VqiE",
        "colab": {}
      },
      "source": [
        "# For collab data\n",
        "samples = 300000\n",
        "# for testing identity mapping\n",
        "#frames = np.memmap(\"ftargets-\"+str(samples), mode='r', dtype=DTYPE, shape=(samples, FFT_BINS, 1))\n",
        "frames = np.memmap(\"fsamples-\"+str(samples), mode='r', dtype=DTYPE, shape=(samples, FFT_BINS, 1))\n",
        "targets = np.memmap(\"ftargets-\"+str(samples), mode='r', dtype=DTYPE, shape=(samples, FFT_BINS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ur6yJmzVkTwc",
        "colab": {}
      },
      "source": [
        "# Code to generate input, target, and verification data\n",
        "# From https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, input_array, target_array, batch_size=32, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.input_array = input_array\n",
        "        self.target_array = target_array\n",
        "        self.dim = (WINDOW_SIZE,FFT_BINS) # Input data shape\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs  # the set of allowed IDs to use as addresses for frames and target samples\n",
        "        self.n_channels = 1\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size, FFT_BINS), dtype=DTYPE)\n",
        "\n",
        "        # Generate data\n",
        "        half_win = WINDOW_SIZE // 2\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample & target\n",
        "            X[i,] = self.input_array[ID-half_win:ID+half_win+1,:,:]\n",
        "            y[i] = self.target_array[ID,:]\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJq_FBWlVqiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare the generator\n",
        "validation_split = 0.05\n",
        "batch_size = 64\n",
        "\n",
        "# Available IDs\n",
        "all_IDs = np.arange(WINDOW_SIZE//2, frames.shape[0]-WINDOW_SIZE//2)\n",
        "np.random.shuffle(all_IDs)\n",
        "\n",
        "validation_cut = int(np.floor(len(all_IDs) * (1-validation_split)))\n",
        "validation_IDs = all_IDs[validation_cut:]\n",
        "train_IDs = all_IDs[0:validation_cut-1]\n",
        "\n",
        "training_generator = DataGenerator(train_IDs, frames, targets, batch_size=batch_size)\n",
        "validation_generator = DataGenerator(validation_IDs, frames, targets, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ibbfUgj5kTwf",
        "scrolled": false,
        "outputId": "8fc4b41a-eeb9-4286-91ab-6bec40573eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Real model (eventually)\n",
        "ALPHA = 0.05\n",
        "L2Reg = 0.0000 # bumping this up to 0.000005 didn't seem very good\n",
        "\n",
        "inputs = layers.Input(shape=(WINDOW_SIZE,FFT_BINS,1)) # Full window\n",
        "\n",
        "NARROW_VIEW = 15\n",
        "\n",
        "# WARNING - if I put a constant in the lambda function, it causes an error when loading the model\n",
        "def sub_samples(samples, n, window_size):\n",
        "    half_cut = (window_size - n) // 2\n",
        "    return samples[:,half_cut:-half_cut,:,:]\n",
        "    \n",
        "# process inputs into subsets of interest\n",
        "input_slice = layers.Lambda(sub_samples, arguments = {'n':1, 'window_size':WINDOW_SIZE}, name=\"targetslice\")(inputs)\n",
        "narrow_view = layers.Lambda(sub_samples, arguments = {'n':NARROW_VIEW, 'window_size':WINDOW_SIZE}, name=\"narrow_view\")(inputs)\n",
        "\n",
        "# zoomed out view\n",
        "broad_pool1 = layers.AveragePooling2D(pool_size=(3,5), strides=(2,3), name=\"broad_pool1\")(inputs)\n",
        "broad_conv1 = layers.Conv2D(64, kernel_size=(5,BINS_PER_OCTAVE//5), strides=(2,BINS_PER_OCTAVE//20), activity_regularizer=regularizers.l2(L2Reg), name=\"broad_conv1\")(broad_pool1)\n",
        "bcn1 = layers.BatchNormalization()(broad_conv1)\n",
        "bc1a = layers.LeakyReLU(alpha=ALPHA)(bcn1)\n",
        "broad_conv2 = layers.Conv2D(32, kernel_size=(3,4), strides=(1,4), activity_regularizer=regularizers.l2(L2Reg), name=\"broad_conv2\")(bc1a)\n",
        "bcn2 = layers.BatchNormalization()(broad_conv2)\n",
        "bc2a = layers.LeakyReLU(alpha=ALPHA)(bcn2)\n",
        "broad_pool2 = layers.MaxPooling2D(pool_size=(1,4), strides=(1,2), name=\"broad_pool2\")(bc2a)\n",
        "bp2a = layers.LeakyReLU(alpha=ALPHA)(broad_pool2)\n",
        "flat_b = layers.Flatten(name=\"flat_b\")(bp2a)\n",
        "\n",
        "# narrow view\n",
        "conv1 = layers.Conv2D(128, kernel_size=(3,BINS_PER_OCTAVE), strides=(1,BINS_PER_OCTAVE//4), activity_regularizer=regularizers.l2(L2Reg), name=\"conv1\")(narrow_view)\n",
        "cn1 = layers.BatchNormalization()(conv1)\n",
        "c1a = layers.LeakyReLU(alpha=ALPHA)(cn1)\n",
        "conv2 = layers.Conv2D(64, kernel_size=(3,4), strides=(1,2), activity_regularizer=regularizers.l2(L2Reg), name=\"conv2\")(c1a)\n",
        "cn2 = layers.BatchNormalization()(conv2)\n",
        "c2a = layers.LeakyReLU(alpha=ALPHA)(cn2)\n",
        "pool1 = layers.MaxPooling2D(pool_size=(1,8), strides=(1,4), name=\"pool1\")(c2a)\n",
        "p1a = layers.LeakyReLU(alpha=ALPHA)(pool1)\n",
        "flat_n = layers.Flatten(name=\"flat_n\")(p1a)\n",
        "\n",
        "# Vertical features - across samples then bins\n",
        "#conv3 = layers.Conv2D(64, kernel_size=(3,30), activity_regularizer=regularizers.l2(L2Reg), name=\"conv3\")(inputs) \n",
        "#c3a = layers.LeakyReLU(alpha=ALPHA)(conv3)\n",
        "#flat_v = layers.Flatten(name=\"flat_v\")(c3a)\n",
        "\n",
        "flat = layers.concatenate([flat_b, flat_n], name=\"merge_modes\")\n",
        "\n",
        "flat_in = layers.Flatten(name=\"flatten\")(input_slice)\n",
        "\n",
        "dense0 = layers.Dense(1000, activity_regularizer=regularizers.l2(L2Reg))(flat)\n",
        "d0a = layers.LeakyReLU(alpha=ALPHA)(dense0)\n",
        "merge1 = layers.concatenate([d0a, flat_in], name=\"merge_input\")\n",
        "\n",
        "dense1 = layers.Dense(300, activity_regularizer=regularizers.l2(L2Reg))(merge1)\n",
        "d1a = keras.layers.LeakyReLU(alpha=ALPHA)(dense1)\n",
        "#dense2 = layers.Dense(500, )(dense1)\n",
        "#d2a = keras.layers.LeakyReLU(alpha=ALPHA)(dense2)\n",
        "merged = layers.concatenate([d1a, flat_in])\n",
        "outputs = layers.Dense(FFT_BINS, activation='linear')(merged)\n",
        "\n",
        "model = keras.models.Model(inputs=[inputs], outputs=outputs)\n",
        "\n",
        "model.compile(optimizer='Adam', loss='mse')\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 55, 513, 1)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "broad_pool1 (AveragePooling2D)  (None, 27, 170, 1)   0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "narrow_view (Lambda)            (None, 15, 513, 1)   0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "broad_conv1 (Conv2D)            (None, 12, 38, 64)   6144        broad_pool1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 13, 18, 128)  36992       narrow_view[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 12, 38, 64)   256         broad_conv1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 13, 18, 128)  512         conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 12, 38, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 13, 18, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "broad_conv2 (Conv2D)            (None, 10, 9, 32)    24608       leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2 (Conv2D)                  (None, 11, 8, 64)    98368       leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 10, 9, 32)    128         broad_conv2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 11, 8, 64)    256         conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 10, 9, 32)    0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 11, 8, 64)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "broad_pool2 (MaxPooling2D)      (None, 10, 3, 32)    0           leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 11, 1, 64)    0           leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 10, 3, 32)    0           broad_pool2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 11, 1, 64)    0           pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flat_b (Flatten)                (None, 960)          0           leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flat_n (Flatten)                (None, 704)          0           leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "merge_modes (Concatenate)       (None, 1664)         0           flat_b[0][0]                     \n",
            "                                                                 flat_n[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1000)         1665000     merge_modes[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "targetslice (Lambda)            (None, 1, 513, 1)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 1000)         0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 513)          0           targetslice[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "merge_input (Concatenate)       (None, 1513)         0           leaky_re_lu_7[0][0]              \n",
            "                                                                 flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 300)          454200      merge_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 300)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 813)          0           leaky_re_lu_8[0][0]              \n",
            "                                                                 flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 513)          417582      concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 2,704,046\n",
            "Trainable params: 2,703,470\n",
            "Non-trainable params: 576\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG4Ie6wWVqiQ",
        "colab_type": "code",
        "outputId": "fc7e31d4-3843-4696-e79a-753e072f8d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# fit_generator\n",
        "stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=20, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
        "save_best = keras.callbacks.ModelCheckpoint(\"Best_model.h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "hist = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=10, callbacks = [stopper, save_best])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4452/4452 [==============================] - 76s 17ms/step - loss: 0.0322 - val_loss: 0.0353\n",
            "Epoch 2/10\n",
            "4452/4452 [==============================] - 76s 17ms/step - loss: 0.0312 - val_loss: 0.0350\n",
            "Epoch 3/10\n",
            "4452/4452 [==============================] - 76s 17ms/step - loss: 0.0299 - val_loss: 0.0342\n",
            "Epoch 4/10\n",
            "4452/4452 [==============================] - 76s 17ms/step - loss: 0.0290 - val_loss: 0.0348\n",
            "Epoch 5/10\n",
            "4452/4452 [==============================] - 76s 17ms/step - loss: 0.0283 - val_loss: 0.0334\n",
            "Epoch 6/10\n",
            "4452/4452 [==============================] - 76s 17ms/step - loss: 0.0275 - val_loss: 0.0304\n",
            "Epoch 7/10\n",
            "4452/4452 [==============================] - 76s 17ms/step - loss: 0.0269 - val_loss: 0.0311\n",
            "Epoch 8/10\n",
            "4452/4452 [==============================] - 76s 17ms/step - loss: 0.0263 - val_loss: 0.0326\n",
            "Epoch 9/10\n",
            "4452/4452 [==============================] - 76s 17ms/step - loss: 0.0258 - val_loss: 0.0322\n",
            "Epoch 10/10\n",
            "4452/4452 [==============================] - 76s 17ms/step - loss: 0.0253 - val_loss: 0.0319\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPctpbJIg6rC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "now = time.strftime('%Y-%m-%d_%H-%M')\n",
        "model.save(\"model_\"+now+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqG_dSiTkTwj",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "923224c3-15b6-45f8-8664-33b7383f84fc"
      },
      "source": [
        "history = hist\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.ylim((0, 2 * history.history['loss'][0])) # because sometimes there are stupid spikes in error\n",
        "plt.show()\n",
        "\n",
        "#plt.savefig('test.png')\n",
        "print(\"Validation loss mean 5+ epochs {0:.4}\".format(np.mean(history.history['val_loss'][5:])))\n",
        "print(\"Lowest validation loss {0:.4}\".format(np.min(history.history['val_loss'][5:])))\n",
        "print(\"Frames \", frames.shape[0])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBYSIiERSQIiISCQFhIiIRPr/uTI4ntQ4D5QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Validation loss mean 5+ epochs 0.03165\n",
            "Lowest validation loss 0.03043\n",
            "Frames  300000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-lCNToBxkTwR",
        "colab": {}
      },
      "source": [
        "# some test data to hack around with\n",
        "test_file = \"Assets\\\\DataShareArchive\\\\Test\\\\Clean\\\\p232_010.wav\"\n",
        "wav, rate = librosa.core.load(test_file)\n",
        "fft = get_ft(wav)\n",
        "#print(fft.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c8DZfaSgkTwn",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# Full round trip test\n",
        "file = \"p232_001.wav\"\n",
        "\n",
        "verify_file = \"Assets\\\\DataShareArchive\\\\Test\\\\Noisy\\\\\" + file\n",
        "clean_file = \"Assets\\\\DataShareArchive\\\\Test\\\\Clean\\\\\" + file\n",
        "\n",
        "\n",
        "# Sample output is (samples, bins) all converted to magnitude\n",
        "def get_samples(file):\n",
        "    wav, rate = librosa.core.load(file)\n",
        "    samples = abs(get_ft(wav).T) # organized as bins, frames so we need to transpose them to frames, bins\n",
        "    return samples\n",
        "\n",
        "# Only need this for testing at this point?  Could still make it a 'top' type shared function\n",
        "def clip_frames(file):\n",
        "    samples = get_samples(file)\n",
        "    print(samples.shape)\n",
        "    frames = np.empty((samples.shape[0], WINDOW_SIZE, FFT_BINS, 1))\n",
        "    half_win = WINDOW_SIZE//2\n",
        "    for i in range(half_win, samples.shape[0]-half_win):\n",
        "        frames[i,:,:,0] = samples[i-half_win:i+half_win+1,:]\n",
        "    return frames\n",
        "\n",
        "\n",
        "def clean_clip(model, n_file):\n",
        "    verify_frames = clip_frames(n_file)\n",
        "    output_targets = model.predict([verify_frames])\n",
        "    wav, rate = librosa.core.load(n_file)\n",
        "    n_fft = get_ft(wav)\n",
        "    fft = rebuild_fft(output_targets, n_fft)\n",
        "    return fft, inv_ft(fft)\n",
        "\n",
        "\n",
        "p_fft, p_wav = clean_clip(model, verify_file)\n",
        "    \n",
        "wav, rate = librosa.core.load(clean_file)\n",
        "c_fft = get_ft(wav)\n",
        "\n",
        "\n",
        "def display_fft(ft):\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(ft), ref=np.max), y_axis='cqt_hz', x_axis='time')\n",
        "\n",
        "print(\"Cleaned clip\")\n",
        "\n",
        "err_fft = c_fft - p_fft\n",
        "print(\"Average abs err vs clean = \", np.mean(abs(err_fft)))\n",
        "\n",
        "display_fft(p_fft)\n",
        "Audio(p_wav,rate=22050)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sqHS_31nkTwp",
        "colab": {}
      },
      "source": [
        "def draw(wav):\n",
        "    fft = get_ft(wav)\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(fft), ref=np.max), y_axis='cqt_hz', x_axis='time')\n",
        "    return fft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3xhtGoYOkTwr",
        "colab": {}
      },
      "source": [
        "wav, rate = librosa.core.load(verify_file)\n",
        "n_fft = draw(wav)\n",
        "\n",
        "err_fft = c_fft - n_fft\n",
        "print(\"Average abs err vs clean = \", np.mean(abs(err_fft)))\n",
        "\n",
        "print(\"Noisy file\")\n",
        "Audio(wav, rate=rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeyY6RDTkTwu",
        "colab": {}
      },
      "source": [
        "wav, rate = librosa.core.load(clean_file)\n",
        "c_fft = draw(wav)\n",
        "\n",
        "print(\"Clean sample\")\n",
        "Audio(wav, rate=rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zO4WwsB9kTww",
        "colab": {}
      },
      "source": [
        "cut_fft = n_fft-p_fft\n",
        "display_fft(cut_fft)\n",
        "\n",
        "print(\"Removed audio\")\n",
        "print(\"Average cut value = \", np.mean(abs(cut_fft)))\n",
        "\n",
        "cut_wav = inv_ft(cut_fft)\n",
        "Audio(cut_wav,rate=22050)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_tC3WpzokTwy",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k7wCtwF4kTw0",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "#%%capture cap\n",
        "print(model.summary())\n",
        "#with open('output.txt', 'w') as f:\n",
        "#    f.write(cap.stdout)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0b_mGnNZkTw3",
        "colab": {}
      },
      "source": [
        "fresh_wav, rate = librosa.core.load(\"Assets\\\\DataShareArchive\\\\Test\\\\Noisy\\\\p232_019.wav\")\n",
        "Audio(fresh_wav, rate=rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "peLEO0VZkTw6"
      },
      "source": [
        "Test & reference stuff "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BPX3_ZiMkTw7",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "ft = clip_frames(test_file)\n",
        "tt = clip_targets(test_file)\n",
        "\n",
        "print(ft.shape)\n",
        "print(tt.shape)\n",
        "\n",
        "#Frame / target check  -  don't use first row because it might be zeroed out\n",
        "r = 10\n",
        "print(tt[0,r])\n",
        "print(ft[0,r,TARGET_COL:TARGET_COL+1,0])\n",
        "\n",
        "# Round trip test\n",
        "new_fft = rebuild_fft(tt, fft)\n",
        "print(new_fft.shape)\n",
        "new_wav = inv_ft(new_fft)\n",
        "\n",
        "librosa.display.specshow(librosa.amplitude_to_db(np.abs(new_fft), ref=np.max), y_axis='log', x_axis='time')\n",
        "\n",
        "Audio(new_wav,rate=22050)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k67NuIJQkTw-",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "    \n",
        "def show_fft(wav):\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(librosa.stft(wav)), ref=np.max), y_axis='log', x_axis='time')\n",
        "\n",
        "def display_fft(ft):\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(fft), ref=np.max), y_axis='log', x_axis='time')\n",
        "\n",
        "librosa.display.specshow(librosa.amplitude_to_db(np.abs(new_fft), ref=np.max), y_axis='log', x_axis='time')\n",
        "\n",
        "print(new_fft.shape)\n",
        "\n",
        "#show_fft(wav)\n",
        "#show_fft(new_wav)\n",
        "#Audio(wav, rate=22050)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M4HRcM0rkTxA",
        "colab": {}
      },
      "source": [
        "# Test stuff for hacking around    \n",
        "    \n",
        "a = np.array([[1+2j, 3+4j, 1.5+2.5j],[5+6j,7+8j, 5.5+6.5j]])\n",
        "print(a)\n",
        "a1 = a[0]\n",
        "print(a1)\n",
        "ar = a1.real\n",
        "ai = a1.imag\n",
        "\n",
        "st = np.zeros((6))\n",
        "st[0:3] = ar\n",
        "st[3:6] = ai\n",
        "print(ar.shape, ai.shape, st.shape)\n",
        "print(ar, ai)\n",
        "print(st)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lofx2hiEkTxC",
        "colab": {}
      },
      "source": [
        "# ''do nothing' model that should be able to guess outputs from outputs\n",
        "\n",
        "inputs = layers.Input(shape=(FFT_BINS*2,))\n",
        "dense1 = layers.Dense(2000, activation='relu')(inputs)\n",
        "dense2 = layers.Dense(2000, activation='relu')(dense1)\n",
        "#flat = layers.Flatten()(dense)\n",
        "#flat_in = layers.Flatten()(inputs)\n",
        "#merged = layers.concatenate([flat, flat_in])\n",
        "merged = layers.concatenate([dense2, inputs])\n",
        "linear = layers.Dense(2000, activation='linear')(merged)\n",
        "outputs = layers.Dense(2*FFT_BINS, activation='linear')(linear)\n",
        "\n",
        "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer='Adam', loss='mse')\n",
        "\n",
        "model.fit(targets, targets, epochs=1, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NN Audio Test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ugJNGnNlkTwI",
        "outputId": "5e78447f-8926-4c7e-df11-56720457c307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import librosa.display\n",
        "from IPython.display import Audio, display\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import time\n",
        "import cmath\n",
        "\n",
        "import keras\n",
        "import keras.layers as layers\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "from keras import regularizers\n",
        "from keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cPWfsAnkkgxD",
        "outputId": "15adcc7c-e624-4c8a-d814-8023252de0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# If on google colab, run this\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('drive/My Drive/Projects/Audio Separation/Data')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X4ZOOJsSkTwM"
      },
      "source": [
        "Rewriten to use a single stream of data and on the fly creation of batch data\n",
        "\n",
        "TODO\n",
        "* Add saving best models\n",
        "\n",
        "* Change name from frames to samples everywhere\n",
        "\n",
        "* Go back to cqt??\n",
        "\n",
        "* Add drop out\n",
        "\n",
        "* Use callbacks for LR reduction and to save best models with error info\n",
        "\n",
        "* Automatic logging of graphs, errors, models etc. for comparison\n",
        "* Add graph of model\n",
        "    from keras.utils import plot_model\n",
        "    plot_model(model, to_file='model.png')\n",
        "* Create log output function, then capture to text file (also export graphs?)\n",
        "\n",
        "* Look for sources of noise that are close to what I want?  Or generate some clips?\n",
        "\n",
        "* Debugging tools for comparing, listening to, and viewing clips\n",
        "* Add audio quality comparisons between clean clips and cleaned clips for evaluation\n",
        "\n",
        "* Add history error plotting to compare different learning models, topologies, etc.\n",
        "\n",
        "* Look into streaming frames to & from file and having way more samples\n",
        "\n",
        "* Why does https://towardsdatascience.com/audio-ai-isolating-vocals-from-stereo-music-using-convolutional-neural-networks-210532383785 only use real component?\n",
        "* Evaluate different representations of complex numbers in terms of learnability\n",
        "\n",
        "* Consider generating custom data for goal - male speakers, low voice, specific sounds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a0NWhq04kTwN",
        "colab": {}
      },
      "source": [
        "# Constants and settings\n",
        "WINDOW_SIZE = 35  # Has to be odd\n",
        "TARGET_COL = WINDOW_SIZE//2\n",
        "\n",
        "DTYPE = 'float32'\n",
        "\n",
        "# cqt related\n",
        "FFT_BINS = 768 # function of items below\n",
        "HOP_LENGTH = 512\n",
        "\n",
        "# stft values\n",
        "N_FFT = 1024 # 512 recommended for speech, music typically 2048\n",
        "\n",
        "# cqt values\n",
        "BINS_PER_OCTAVE = 12 * 8\n",
        "FMIN = librosa.note_to_hz('C1')\n",
        "OCTAVES = 8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MUhAB8HrkTwP",
        "colab": {}
      },
      "source": [
        "# Idea here is that we operate on magnitude, and will just use phase from the original noisy sample\n",
        "\n",
        "def combine_target(t):\n",
        "    return (t[0:t.shape[0]//2] + 1j * t[t.shape[0]//2:]).reshape(1,(t.shape[0]//2))\n",
        "\n",
        "def rebuild_fft(output, original_fft):\n",
        "    vphase = np.vectorize(cmath.phase)\n",
        "    o_phase = vphase(original_fft)\n",
        "    mag = output.T\n",
        "    vrect = np.vectorize(cmath.rect)\n",
        "    return vrect(mag, o_phase)\n",
        "    \n",
        "# build up as (bins, samples) then transpose to model view of (samples, bins)\n",
        "def targets_to_fft(targets):\n",
        "    fft = np.empty((targets.shape[0],targets.shape[1]//2), dtype='complex64')\n",
        "    for i in range(0, targets.shape[0]):\n",
        "        fft[i] = combine_target(targets[i])\n",
        "    return fft.T   # transpose\n",
        "\n",
        "def get_ft(wav):\n",
        "    c = librosa.cqt(wav, hop_length=HOP_LENGTH, fmin=FMIN, n_bins=OCTAVES*BINS_PER_OCTAVE, bins_per_octave=BINS_PER_OCTAVE)\n",
        "    #c = librosa.stft(wav, hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
        "    return c\n",
        "\n",
        "def inv_ft(ft):\n",
        "    return librosa.icqt(ft, hop_length=HOP_LENGTH, bins_per_octave=BINS_PER_OCTAVE)\n",
        "    #return librosa.istft(ft, hop_length=HOP_LENGTH)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0mgM7ZVjlStg",
        "colab": {}
      },
      "source": [
        "# For local data\n",
        "samples=100000\n",
        "frames = np.memmap(\"f://audiodata//fsamples-\"+str(samples), mode='r', dtype=DTYPE, shape=(samples, FFT_BINS, 1))\n",
        "targets = np.memmap(\"f://audiodata//ftargets-\"+str(samples), mode='r', dtype=DTYPE, shape=(samples, FFT_BINS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xFRoJVK1VqiE",
        "colab": {}
      },
      "source": [
        "# For collab data\n",
        "samples = 300000\n",
        "frames = np.memmap(\"fsamples-\"+str(samples), mode='r', dtype=DTYPE, shape=(samples, FFT_BINS, 1))\n",
        "targets = np.memmap(\"ftargets-\"+str(samples), mode='r', dtype=DTYPE, shape=(samples, FFT_BINS))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ur6yJmzVkTwc",
        "colab": {}
      },
      "source": [
        "# Code to generate input, target, and verification data\n",
        "# From https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, input_array, target_array, batch_size=32, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.input_array = input_array\n",
        "        self.target_array = target_array\n",
        "        self.dim = (WINDOW_SIZE,FFT_BINS) # Input data shape\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs  # the set of allowed IDs to use as addresses for frames and target samples\n",
        "        self.n_channels = 1\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size, FFT_BINS), dtype=DTYPE)\n",
        "\n",
        "        # Generate data\n",
        "        half_win = WINDOW_SIZE // 2\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample & target\n",
        "            X[i,] = self.input_array[ID-half_win:ID+half_win+1,:,:]\n",
        "            y[i] = self.target_array[ID,:]\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJq_FBWlVqiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare the generator\n",
        "validation_split = 0.05\n",
        "batch_size = 64\n",
        "\n",
        "# Available IDs\n",
        "all_IDs = np.arange(WINDOW_SIZE//2, frames.shape[0]-WINDOW_SIZE//2)\n",
        "np.random.shuffle(all_IDs)\n",
        "\n",
        "validation_cut = int(np.floor(len(all_IDs) * (1-validation_split)))\n",
        "validation_IDs = all_IDs[validation_cut:]\n",
        "train_IDs = all_IDs[0:validation_cut-1]\n",
        "\n",
        "training_generator = DataGenerator(train_IDs, frames, targets, batch_size=batch_size)\n",
        "validation_generator = DataGenerator(validation_IDs, frames, targets, batch_size=batch_size)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ibbfUgj5kTwf",
        "scrolled": false,
        "outputId": "83934000-fe52-4e26-cedf-70752d97115c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Real model (eventually)\n",
        "ALPHA = 0.05\n",
        "L2Reg = 0.00000 # bumping this up to 0.000005 didn't seem very good\n",
        "\n",
        "inputs = layers.Input(shape=(WINDOW_SIZE,FFT_BINS,1)) # Full window\n",
        "\n",
        "NARROW_VIEW = 15\n",
        "\n",
        "# WARNING - if I put a constant in the lambda function, it causes an error when loading the model\n",
        "def sub_samples(samples, n, window_size):\n",
        "    half_cut = (window_size - n) // 2\n",
        "    return samples[:,half_cut:-half_cut,:,:]\n",
        "    \n",
        "# process inputs into subsets of interest\n",
        "input_slice = layers.Lambda(sub_samples, arguments = {'n':1, 'window_size':WINDOW_SIZE}, name=\"targetslice\")(inputs)\n",
        "narrow_view = layers.Lambda(sub_samples, arguments = {'n':NARROW_VIEW, 'window_size':WINDOW_SIZE}, name=\"narrow_view\")(inputs)\n",
        "\n",
        "# zoomed out view\n",
        "broad_pool1 = layers.AveragePooling2D(pool_size=(3,5), strides=(2,3), name=\"broad_pool1\")(inputs)\n",
        "broad_conv1 = layers.Conv2D(64, kernel_size=(5,BINS_PER_OCTAVE//5), strides=(2,BINS_PER_OCTAVE//20), activity_regularizer=regularizers.l2(L2Reg), name=\"broad_conv1\")(broad_pool1)\n",
        "bc1a = layers.LeakyReLU(alpha=ALPHA)(broad_conv1)\n",
        "broad_conv2 = layers.Conv2D(32, kernel_size=(3,4), strides=(1,4), activity_regularizer=regularizers.l2(L2Reg), name=\"broad_conv2\")(bc1a)\n",
        "bc2a = layers.LeakyReLU(alpha=ALPHA)(broad_conv2)\n",
        "broad_pool2 = layers.MaxPooling2D(pool_size=(1,4), strides=(1,2), name=\"broad_pool2\")(bc2a)\n",
        "bp2a = layers.LeakyReLU(alpha=ALPHA)(broad_pool2)\n",
        "flat_b = layers.Flatten(name=\"flat_b\")(bp2a)\n",
        "\n",
        "# narrow view\n",
        "conv1 = layers.Conv2D(128, kernel_size=(3,BINS_PER_OCTAVE), strides=(1,BINS_PER_OCTAVE//4), activity_regularizer=regularizers.l2(L2Reg), name=\"conv1\")(narrow_view)\n",
        "c1a = layers.LeakyReLU(alpha=ALPHA)(conv1)\n",
        "conv2 = layers.Conv2D(64, kernel_size=(3,4), strides=(1,2), activity_regularizer=regularizers.l2(L2Reg), name=\"conv2\")(c1a)\n",
        "c2a = layers.LeakyReLU(alpha=ALPHA)(conv2)\n",
        "pool1 = layers.MaxPooling2D(pool_size=(1,8), strides=(1,4), name=\"pool1\")(c2a)\n",
        "p1a = layers.LeakyReLU(alpha=ALPHA)(pool1)\n",
        "flat_n = layers.Flatten(name=\"flat_n\")(p1a)\n",
        "\n",
        "# Vertical features - across samples then bins\n",
        "#conv3 = layers.Conv2D(64, kernel_size=(3,30), activity_regularizer=regularizers.l2(L2Reg), name=\"conv3\")(inputs) \n",
        "#c3a = layers.LeakyReLU(alpha=ALPHA)(conv3)\n",
        "#flat_v = layers.Flatten(name=\"flat_v\")(c3a)\n",
        "\n",
        "flat = layers.concatenate([flat_b, flat_n], name=\"merge_modes\")\n",
        "\n",
        "flat_in = layers.Flatten(name=\"flatten\")(input_slice)\n",
        "\n",
        "dense0 = layers.Dense(1000, activity_regularizer=regularizers.l2(L2Reg))(flat)\n",
        "d0a = layers.LeakyReLU(alpha=ALPHA)(dense0)\n",
        "merge1 = layers.concatenate([d0a, flat_in], name=\"merge_input\")\n",
        "\n",
        "dense1 = layers.Dense(300, activity_regularizer=regularizers.l2(L2Reg))(merge1)\n",
        "d1a = keras.layers.LeakyReLU(alpha=ALPHA)(dense1)\n",
        "#dense2 = layers.Dense(500, )(dense1)\n",
        "#d2a = keras.layers.LeakyReLU(alpha=ALPHA)(dense2)\n",
        "merged = layers.concatenate([d1a, flat_in])\n",
        "outputs = layers.Dense(FFT_BINS, activation='linear')(merged)\n",
        "\n",
        "model = keras.models.Model(inputs=[inputs], outputs=outputs)\n",
        "\n",
        "model.compile(optimizer='Adam', loss='mse')\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 45, 768, 1)   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "broad_pool1 (AveragePooling2D)  (None, 22, 255, 1)   0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "narrow_view (Lambda)            (None, 15, 768, 1)   0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "broad_conv1 (Conv2D)            (None, 9, 60, 64)    6144        broad_pool1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1 (Conv2D)                  (None, 13, 29, 128)  36992       narrow_view[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 9, 60, 64)    0           broad_conv1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 13, 29, 128)  0           conv1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "broad_conv2 (Conv2D)            (None, 7, 15, 32)    24608       leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2 (Conv2D)                  (None, 11, 13, 64)   98368       leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 7, 15, 32)    0           broad_conv2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 11, 13, 64)   0           conv2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "broad_pool2 (MaxPooling2D)      (None, 7, 6, 32)     0           leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "pool1 (MaxPooling2D)            (None, 11, 2, 64)    0           leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 7, 6, 32)     0           broad_pool2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 11, 2, 64)    0           pool1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flat_b (Flatten)                (None, 1344)         0           leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flat_n (Flatten)                (None, 1408)         0           leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "merge_modes (Concatenate)       (None, 2752)         0           flat_b[0][0]                     \n",
            "                                                                 flat_n[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1000)         2753000     merge_modes[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "targetslice (Lambda)            (None, 1, 768, 1)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 1000)         0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 768)          0           targetslice[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "merge_input (Concatenate)       (None, 1768)         0           leaky_re_lu_7[0][0]              \n",
            "                                                                 flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 300)          530700      merge_input[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 300)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 1068)         0           leaky_re_lu_8[0][0]              \n",
            "                                                                 flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 768)          820992      concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 4,270,804\n",
            "Trainable params: 4,270,804\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG4Ie6wWVqiQ",
        "colab_type": "code",
        "outputId": "e79b0a71-ec08-42e1-e8d9-abacfd48cc8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "# fit_generator\n",
        "stopper = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=15, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
        "save_best = keras.callbacks.ModelCheckpoint(\"Best_model.h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "hist = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=5, callbacks = [stopper, save_best])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "4452/4452 [==============================] - 182s 41ms/step - loss: 0.0024 - val_loss: 0.0011\n",
            "Epoch 2/5\n",
            "4452/4452 [==============================] - 87s 19ms/step - loss: 9.0635e-04 - val_loss: 7.8675e-04\n",
            "Epoch 3/5\n",
            "4452/4452 [==============================] - 87s 19ms/step - loss: 7.6299e-04 - val_loss: 7.6823e-04\n",
            "Epoch 4/5\n",
            "4452/4452 [==============================] - 87s 19ms/step - loss: 6.8983e-04 - val_loss: 6.8618e-04\n",
            "Epoch 5/5\n",
            "4452/4452 [==============================] - 86s 19ms/step - loss: 6.5406e-04 - val_loss: 6.4508e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPctpbJIg6rC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "now = time.strftime('%Y-%m-%d_%H-%M')\n",
        "model.save(\"model_\"+now+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqG_dSiTkTwj",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "history = hist\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.ylim((0, 2 * history.history['loss'][0])) # because sometimes there are stupid spikes in error\n",
        "plt.show()\n",
        "\n",
        "#plt.savefig('test.png')\n",
        "print(\"Validation loss mean 5+ epochs {0:.4}\".format(np.mean(history.history['val_loss'][5:])))\n",
        "print(\"Lowest validation loss {0:.4}\".format(np.min(history.history['val_loss'][5:])))\n",
        "print(\"Frames \", frames.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1tuZ62OokTwl",
        "colab": {}
      },
      "source": [
        "# Not required because of 'restore best weights'\n",
        "#model = keras.models.load_model(\"Best_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-lCNToBxkTwR",
        "colab": {}
      },
      "source": [
        "# some test data to hack around with\n",
        "test_file = \"Assets\\\\DataShareArchive\\\\Test\\\\Clean\\\\p232_010.wav\"\n",
        "wav, rate = librosa.core.load(test_file)\n",
        "fft = get_ft(wav)\n",
        "#print(fft.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c8DZfaSgkTwn",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "# Full round trip test\n",
        "file = \"p232_001.wav\"\n",
        "\n",
        "verify_file = \"Assets\\\\DataShareArchive\\\\Test\\\\Noisy\\\\\" + file\n",
        "clean_file = \"Assets\\\\DataShareArchive\\\\Test\\\\Clean\\\\\" + file\n",
        "\n",
        "\n",
        "# Sample output is (samples, bins) all converted to magnitude\n",
        "def get_samples(file):\n",
        "    wav, rate = librosa.core.load(file)\n",
        "    samples = abs(get_ft(wav).T) # organized as bins, frames so we need to transpose them to frames, bins\n",
        "    return samples\n",
        "\n",
        "# Only need this for testing at this point?  Could still make it a 'top' type shared function\n",
        "def clip_frames(file):\n",
        "    samples = get_samples(file)\n",
        "    print(samples.shape)\n",
        "    frames = np.empty((samples.shape[0], WINDOW_SIZE, FFT_BINS, 1))\n",
        "    half_win = WINDOW_SIZE//2\n",
        "    for i in range(half_win, samples.shape[0]-half_win):\n",
        "        frames[i,:,:,0] = samples[i-half_win:i+half_win+1,:]\n",
        "    return frames\n",
        "\n",
        "\n",
        "def clean_clip(model, n_file):\n",
        "    verify_frames = clip_frames(n_file)\n",
        "    output_targets = model.predict([verify_frames])\n",
        "    wav, rate = librosa.core.load(n_file)\n",
        "    n_fft = get_ft(wav)\n",
        "    fft = rebuild_fft(output_targets, n_fft)\n",
        "    return fft, inv_ft(fft)\n",
        "\n",
        "\n",
        "p_fft, p_wav = clean_clip(model, verify_file)\n",
        "    \n",
        "wav, rate = librosa.core.load(clean_file)\n",
        "c_fft = get_ft(wav)\n",
        "\n",
        "\n",
        "def display_fft(ft):\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(ft), ref=np.max), y_axis='cqt_hz', x_axis='time')\n",
        "\n",
        "print(\"Cleaned clip\")\n",
        "\n",
        "err_fft = c_fft - p_fft\n",
        "print(\"Average abs err vs clean = \", np.mean(abs(err_fft)))\n",
        "\n",
        "display_fft(p_fft)\n",
        "Audio(p_wav,rate=22050)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sqHS_31nkTwp",
        "colab": {}
      },
      "source": [
        "def draw(wav):\n",
        "    fft = get_ft(wav)\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(fft), ref=np.max), y_axis='cqt_hz', x_axis='time')\n",
        "    return fft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3xhtGoYOkTwr",
        "colab": {}
      },
      "source": [
        "wav, rate = librosa.core.load(verify_file)\n",
        "n_fft = draw(wav)\n",
        "\n",
        "err_fft = c_fft - n_fft\n",
        "print(\"Average abs err vs clean = \", np.mean(abs(err_fft)))\n",
        "\n",
        "print(\"Noisy file\")\n",
        "Audio(wav, rate=rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeyY6RDTkTwu",
        "colab": {}
      },
      "source": [
        "wav, rate = librosa.core.load(clean_file)\n",
        "c_fft = draw(wav)\n",
        "\n",
        "print(\"Clean sample\")\n",
        "Audio(wav, rate=rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zO4WwsB9kTww",
        "colab": {}
      },
      "source": [
        "cut_fft = n_fft-p_fft\n",
        "display_fft(cut_fft)\n",
        "\n",
        "print(\"Removed audio\")\n",
        "print(\"Average cut value = \", np.mean(abs(cut_fft)))\n",
        "\n",
        "cut_wav = inv_ft(cut_fft)\n",
        "Audio(cut_wav,rate=22050)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_tC3WpzokTwy",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k7wCtwF4kTw0",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "#%%capture cap\n",
        "print(model.summary())\n",
        "#with open('output.txt', 'w') as f:\n",
        "#    f.write(cap.stdout)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0b_mGnNZkTw3",
        "colab": {}
      },
      "source": [
        "fresh_wav, rate = librosa.core.load(\"Assets\\\\DataShareArchive\\\\Test\\\\Noisy\\\\p232_019.wav\")\n",
        "Audio(fresh_wav, rate=rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "peLEO0VZkTw6"
      },
      "source": [
        "Test & reference stuff "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BPX3_ZiMkTw7",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "ft = clip_frames(test_file)\n",
        "tt = clip_targets(test_file)\n",
        "\n",
        "print(ft.shape)\n",
        "print(tt.shape)\n",
        "\n",
        "#Frame / target check  -  don't use first row because it might be zeroed out\n",
        "r = 10\n",
        "print(tt[0,r])\n",
        "print(ft[0,r,TARGET_COL:TARGET_COL+1,0])\n",
        "\n",
        "# Round trip test\n",
        "new_fft = rebuild_fft(tt, fft)\n",
        "print(new_fft.shape)\n",
        "new_wav = inv_ft(new_fft)\n",
        "\n",
        "librosa.display.specshow(librosa.amplitude_to_db(np.abs(new_fft), ref=np.max), y_axis='log', x_axis='time')\n",
        "\n",
        "Audio(new_wav,rate=22050)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k67NuIJQkTw-",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "    \n",
        "def show_fft(wav):\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(librosa.stft(wav)), ref=np.max), y_axis='log', x_axis='time')\n",
        "\n",
        "def display_fft(ft):\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(np.abs(fft), ref=np.max), y_axis='log', x_axis='time')\n",
        "\n",
        "librosa.display.specshow(librosa.amplitude_to_db(np.abs(new_fft), ref=np.max), y_axis='log', x_axis='time')\n",
        "\n",
        "print(new_fft.shape)\n",
        "\n",
        "#show_fft(wav)\n",
        "#show_fft(new_wav)\n",
        "#Audio(wav, rate=22050)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M4HRcM0rkTxA",
        "colab": {}
      },
      "source": [
        "# Test stuff for hacking around    \n",
        "    \n",
        "a = np.array([[1+2j, 3+4j, 1.5+2.5j],[5+6j,7+8j, 5.5+6.5j]])\n",
        "print(a)\n",
        "a1 = a[0]\n",
        "print(a1)\n",
        "ar = a1.real\n",
        "ai = a1.imag\n",
        "\n",
        "st = np.zeros((6))\n",
        "st[0:3] = ar\n",
        "st[3:6] = ai\n",
        "print(ar.shape, ai.shape, st.shape)\n",
        "print(ar, ai)\n",
        "print(st)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lofx2hiEkTxC",
        "colab": {}
      },
      "source": [
        "# ''do nothing' model that should be able to guess outputs from outputs\n",
        "\n",
        "inputs = layers.Input(shape=(FFT_BINS*2,))\n",
        "dense1 = layers.Dense(2000, activation='relu')(inputs)\n",
        "dense2 = layers.Dense(2000, activation='relu')(dense1)\n",
        "#flat = layers.Flatten()(dense)\n",
        "#flat_in = layers.Flatten()(inputs)\n",
        "#merged = layers.concatenate([flat, flat_in])\n",
        "merged = layers.concatenate([dense2, inputs])\n",
        "linear = layers.Dense(2000, activation='linear')(merged)\n",
        "outputs = layers.Dense(2*FFT_BINS, activation='linear')(linear)\n",
        "\n",
        "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer='Adam', loss='mse')\n",
        "\n",
        "model.fit(targets, targets, epochs=1, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}